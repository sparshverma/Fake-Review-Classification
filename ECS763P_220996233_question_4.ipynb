{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BrvQ8v1nztO"
      },
      "outputs": [],
      "source": [
        "import csv                               # csv reader\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from IPython.display import display\n",
        "from nltk.corpus import stopwords\n",
        "from random import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from collections import Counter\n",
        "import numpy\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN0tdy3-nztP"
      },
      "outputs": [],
      "source": [
        "# load data from a file and append it to the rawData\n",
        "def loadData(path, Text=None):\n",
        "    with open(path, encoding=\"utf8\") as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader, None)\n",
        "        for line in reader:\n",
        "            (Id, Text, Rating, Verified, ProductID, Label) = parseReview(line)\n",
        "            rawData.append((Id, Text, Rating, Verified, ProductID, Label))\n",
        "            preprocessedData.append((Id, preProcess(Text, Rating, Verified, ProductID), Label))\n",
        "\n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Text, Rating, Verified, ProductID, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(preProcess(Text, Rating, Verified, ProductID)),Label))\n",
        "    for (_, Text, Rating, Verified, ProductID, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(preProcess(Text, Rating, Verified, ProductID)),Label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Awr0vQnztQ"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTKlFv4ynztQ"
      },
      "outputs": [],
      "source": [
        "# Convert line from input file into an id/text/label tuple\n",
        "def parseReview(reviewLine):\n",
        "  ID = 0\n",
        "  TEXT = 8\n",
        "  LABEL = 1\n",
        "  RATING = 2\n",
        "  VERIFIED_PURCHASE = 3\n",
        "  PRODUCT_ID = 5\n",
        "  tuple = (int(reviewLine[ID]), reviewLine[TEXT], int(reviewLine[RATING]), True if reviewLine[VERIFIED_PURCHASE] == 'Y' else False, reviewLine[PRODUCT_ID], fakeLabel if reviewLine[LABEL]=='__label1__' else realLabel)\n",
        "  return tuple\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "    # return (None, None, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9o_JM88nztR"
      },
      "outputs": [],
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "\n",
        "# Input: a string of one review\n",
        "def preProcess(text, rating, verified, product_id):\n",
        "    # Tokenizing using nltk's word_tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Removing URLs\n",
        "    tokens = [re.sub(r'http\\S+', '', tok) for tok in tokens]\n",
        "\n",
        "    # Removing stopwords\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Removing punctuations\n",
        "    filtered_tokens = [tok for tok in filtered_tokens if tok not in string.punctuation]\n",
        "\n",
        "    # Stemming and lemmatisation\n",
        "    filtered_tokens = [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(tok)) for tok in filtered_tokens]\n",
        "\n",
        "    return (filtered_tokens, rating, verified, product_id)\n",
        "    # Should return a list of tokens\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "    # return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtj0U6S9nztR"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onWAN4N0nztR"
      },
      "outputs": [],
      "source": [
        "featureDict = {} # A global dictionary of features\n",
        "\n",
        "def toFeatureVector(tokens):\n",
        "  featurevect = dict(Counter(tokens[0]))\n",
        "  featurevect['rating'] = tokens[1]\n",
        "  featurevect['verified'] = int(tokens[2])\n",
        "  # featurevect['productID'+tokens[3]] = 1\n",
        "  return featurevect\n",
        "    # Should return a dictionary containing features as keys, and weights as values\n",
        "    # DESCRIBE YOUR METHOD IN WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jat5f7hunztR"
      },
      "outputs": [],
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "def trainClassifier(trainData):\n",
        "    print(\"Training Classifier...\")\n",
        "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
        "    return SklearnClassifier(pipeline).train(trainData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOMwGBAnztR"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7jSXQqenztR"
      },
      "outputs": [],
      "source": [
        "def crossValidate(dataset, folds):\n",
        "    shuffle(dataset)\n",
        "    cv_results = []\n",
        "    foldSize = int(len(dataset)/folds)\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "    for i in range(0,len(dataset),foldSize):\n",
        "       testFold = dataset[i:i+foldSize] # This our fold used for testing.\n",
        "       trainingData = dataset[0:i] + dataset[foldSize:]\n",
        "       classifier = trainClassifier(trainingData)\n",
        "       y_pred = predictLabels(testFold, classifier)\n",
        "       y_true = list(map(lambda t: t[1], testFold))\n",
        "       results = list(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
        "       results[3] = accuracy_score(y_true, y_pred) * 100\n",
        "       cv_results.append(tuple(results))\n",
        "\n",
        "    return cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_7o40sJnztR"
      },
      "outputs": [],
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predictLabels(reviewSamples, classifier):\n",
        "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
        "\n",
        "def predictLabel(reviewSample, classifier):\n",
        "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Run this once\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXa0VSQ7sbB3",
        "outputId": "1e57aa14-fcdb-4225-ddb5-089a4ad805db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IDjRgw6YnztS",
        "outputId": "e41e76ab-5763-478d-95a5-a8d16ade2f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "0\n",
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[(0.7721106152024811,\n",
              "  0.7720238095238096,\n",
              "  0.7720538064867261,\n",
              "  77.20238095238095),\n",
              " (0.9846188421103192,\n",
              "  0.9845238095238096,\n",
              "  0.9845210441900337,\n",
              "  98.45238095238096),\n",
              " (0.9738323661777428,\n",
              "  0.9738095238095238,\n",
              "  0.9738085215078749,\n",
              "  97.38095238095238),\n",
              " (0.977982942734907,\n",
              "  0.9779761904761904,\n",
              "  0.9779764479935069,\n",
              "  97.79761904761905),\n",
              " (0.9792030456277213,\n",
              "  0.9791666666666666,\n",
              "  0.9791678552988425,\n",
              "  97.91666666666666),\n",
              " (0.9774173913043478,\n",
              "  0.9773809523809524,\n",
              "  0.977376971957069,\n",
              "  97.73809523809524),\n",
              " (0.9815640292261484, 0.981547619047619, 0.9815471286710225, 98.1547619047619),\n",
              " (0.9792020093793713,\n",
              "  0.9791666666666666,\n",
              "  0.979167338416546,\n",
              "  97.91666666666666),\n",
              " (0.9774288751645237,\n",
              "  0.9773809523809524,\n",
              "  0.9773827480422567,\n",
              "  97.73809523809524),\n",
              " (0.9714533530857719,\n",
              "  0.9714285714285714,\n",
              "  0.9714285714285714,\n",
              "  97.14285714285714)]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "# initialize global lists that will be appended to by the methods below\n",
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = []\n",
        "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "# Do the actual stuff (i.e. call the functions we've made)\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath)\n",
        "\n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "# You do the cross validation on the 80% (training data)\n",
        "# We print the number of training samples and the number of features before the split\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features after the split\n",
        "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "cvResults = crossValidate(trainData, 10)\n",
        "display(cvResults)\n",
        "\n",
        "print(\"Done!\")\n",
        "# QUESTION 3 - Make sure there is a function call here to the\n",
        "# crossValidate function on the training set to get your results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL3K4p8jnztS"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAdl-urinztS",
        "outputId": "ac781fda-c32e-4083-a210-5f84123b3da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'assort': 1, 'realli': 1, 'hershey': 1, \"'s\": 1, 'best': 1, 'littl': 1, 'one': 1, 'alway': 1, 'excit': 1, 'whenev': 1, 'holiday': 1, 'come': 1, 'rating': 5, 'verified': 0}, 'fake')\n",
            "Training Classifier...\n",
            "Done training!\n",
            "Precision: 0.740428\n",
            "Recall: 0.740238\n",
            "F Score:0.740187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Finally, check the accuracy of your classifier by training on all the traning data\n",
        "# and testing on the test set\n",
        "# Will only work once all functions are complete\n",
        "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
        "if functions_complete:\n",
        "    print(testData[0])   # have a look at the first test data instance\n",
        "    classifier = trainClassifier(trainData)  # train the classifier\n",
        "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
        "    testPred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
        "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
        "    print(\"Done training!\")\n",
        "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI8vVO7TnztT"
      },
      "source": [
        "# Questions 4\n",
        "\n",
        "* Code Duplication: In the preProcess function, we are filtering stopwords twice. This redundancy can be eliminated.\n",
        "\n",
        "* Tokenization: Instead of simply using split(\" \") for tokenization, we will be using a more sophisticated tokenizer like nltk.word_tokenize(). This will handle punctuations and other delimiters better.\n",
        "\n",
        "* Handling Punctuations: The code attempts to remove punctuation using string.punctuation but doesn't use the str.translate method properly. Changing it to a regex-based approach.\n",
        "\n",
        "* Feature Extraction: When converting to a feature vector, the product ID is used directly. Now prefixing it with a specific string (e.g., \"PRODUCT_\") to distinguish it from other features.\n",
        "\n",
        "* Numerical Features Normalization: Features like 'rating' might benefit from normalization to ensure they're on a similar scale as other features. Now scaling the ratings between 0 and 1.\n",
        "\n",
        "* Extend Stopword Removal: Reviews might contain common words specific to product reviews that don't contribute much to the classification. Words/phrases like \"product\", \"amazon\", \"purchase\" might be common and not particularly informative.\n",
        "\n",
        "* Preservation and Removal:\n",
        "   1. Preserve emoticons: Reviews might have emoticons, which can be quite informative about the sentiment.\n",
        "   2. Remove URLs: They are probably not informative for classification.\n",
        "   3. We should be removing numbers or handling them differently.\n",
        "\n"
      ]
    }
  ]
}